{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUtvxW4p9sQ3",
        "outputId": "b385a888-7922-4fb4-c7fb-54473a833453"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.96)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.20.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.8.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.11.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.1.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install sentencepiece\n",
        "!pip install transformers\n",
        "!pip install torch\n",
        "\n",
        "# Importing libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "import os\n",
        "\n",
        "# Importing the T5 modules from huggingface/transformers\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "# Setting up the device for GPU usage\n",
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "hJt9FsQU_FJ-"
      },
      "outputs": [],
      "source": [
        "class Chatbot(Dataset):\n",
        "    \"\"\"\n",
        "    Creating a custom dataset for reading the dataset and\n",
        "    loading it into the dataloader to pass it to the\n",
        "    neural network for finetuning the model\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, dataframe, tokenizer, source_len, target_len, source_text, target_text\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initializes a Dataset class\n",
        "\n",
        "        Args:\n",
        "            dataframe (pandas.DataFrame): Input dataframe\n",
        "            tokenizer (transformers.tokenizer): Transformers tokenizer\n",
        "            source_len (int): Max length of source text\n",
        "            target_len (int): Max length of target text\n",
        "            source_text (str): column name of source text\n",
        "            target_text (str): column name of target text\n",
        "        \"\"\"\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.source_len = source_len\n",
        "        self.summ_len = target_len\n",
        "        self.target_text = self.data[target_text]\n",
        "        self.source_text = self.data[source_text]\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"returns the length of dataframe\"\"\"\n",
        "\n",
        "        return len(self.target_text)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"return the input ids, attention masks and target ids\"\"\"\n",
        "\n",
        "        source_text = str(self.source_text[index])\n",
        "        target_text = str(self.target_text[index])\n",
        "\n",
        "        # cleaning data so as to ensure data is in string type\n",
        "        source_text = \" \".join(source_text.split())\n",
        "        target_text = \" \".join(target_text.split())\n",
        "\n",
        "        source = self.tokenizer.batch_encode_plus(\n",
        "            [source_text],\n",
        "            max_length=self.source_len,\n",
        "            pad_to_max_length=True,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        target = self.tokenizer.batch_encode_plus(\n",
        "            [target_text],\n",
        "            max_length=self.summ_len,\n",
        "            pad_to_max_length=True,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        source_ids = source[\"input_ids\"].squeeze()\n",
        "        source_mask = source[\"attention_mask\"].squeeze()\n",
        "        target_ids = target[\"input_ids\"].squeeze()\n",
        "        target_mask = target[\"attention_mask\"].squeeze()\n",
        "\n",
        "        return {\n",
        "            \"source_ids\": source_ids.to(dtype=torch.long),\n",
        "            \"source_mask\": source_mask.to(dtype=torch.long),\n",
        "            \"target_ids\": target_ids.to(dtype=torch.long),\n",
        "            \"target_ids_y\": target_ids.to(dtype=torch.long),\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "P9KcP5DN_ZuN"
      },
      "outputs": [],
      "source": [
        "def train(epoch, tokenizer, model, device, loader, optimizer):\n",
        "\n",
        "    \"\"\"\n",
        "    Function to be called for training with the parameters passed from main function\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    model.train()\n",
        "    for idx, data in enumerate(loader, 0):\n",
        "        y = data[\"target_ids\"].to(device, dtype=torch.long)\n",
        "        y_ids = y[:, :-1].contiguous()\n",
        "        lm_labels = y[:, 1:].clone().detach()\n",
        "        lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
        "        ids = data[\"source_ids\"].to(device, dtype=torch.long)\n",
        "        mask = data[\"source_mask\"].to(device, dtype=torch.long)\n",
        "\n",
        "        outputs = model(\n",
        "            input_ids=ids,\n",
        "            attention_mask=mask,\n",
        "            decoder_input_ids=y_ids,\n",
        "            labels=lm_labels,\n",
        "        )\n",
        "        loss = outputs[0]\n",
        "\n",
        "        if idx % 10 == 0:\n",
        "          print('idx:', idx, 'epoch:', epoch, 'loss:', loss)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "XLwk9zfE_hHV"
      },
      "outputs": [],
      "source": [
        "def validate(epoch, tokenizer, model, device, loader):\n",
        "\n",
        "  \"\"\"\n",
        "  Function to evaluate model for predictions\n",
        "\n",
        "  \"\"\"\n",
        "  model.eval()\n",
        "  predictions = []\n",
        "  actuals = []\n",
        "  with torch.no_grad():\n",
        "      for idx, data in enumerate(loader, 0):\n",
        "          y = data['target_ids'].to(device, dtype = torch.long)\n",
        "          ids = data['source_ids'].to(device, dtype = torch.long)\n",
        "          mask = data['source_mask'].to(device, dtype = torch.long)\n",
        "\n",
        "          generated_ids = model.generate(\n",
        "              input_ids = ids,\n",
        "              attention_mask = mask, \n",
        "              max_length=150, \n",
        "              num_beams=2,\n",
        "              repetition_penalty=2.5, \n",
        "              length_penalty=1.0, \n",
        "              early_stopping=True\n",
        "              )\n",
        "          preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
        "          target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]\n",
        "          if idx % 10 == 0:\n",
        "            print(f'Completed {idx}')\n",
        "\n",
        "          predictions.extend(preds)\n",
        "          actuals.extend(target)\n",
        "  return predictions, actuals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "jTecwtFz_itV"
      },
      "outputs": [],
      "source": [
        "def T5Trainer(\n",
        "    dataframe, source_text, target_text, model_params, output_dir=\"./outputs/\"\n",
        "):\n",
        "\n",
        "    \"\"\"\n",
        "    T5 trainer\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # Set random seeds and deterministic pytorch for reproducibility\n",
        "    torch.manual_seed(model_params[\"SEED\"])  # pytorch random seed\n",
        "    np.random.seed(model_params[\"SEED\"])  # numpy random seed\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "    print(f\"\"\"[Model]: Loading {model_params[\"MODEL\"]}...\\n\"\"\")\n",
        "\n",
        "    # tokenzier for encoding the text\n",
        "    tokenizer = T5Tokenizer.from_pretrained(model_params[\"MODEL\"])\n",
        "\n",
        "    # Defining the model. We are using t5-base model and added a Language model layer on top for generation of Summary.\n",
        "    # Further this model is sent to device (GPU/TPU) for using the hardware.\n",
        "    model = T5ForConditionalGeneration.from_pretrained(model_params[\"MODEL\"])\n",
        "    model = model.to(device)\n",
        "\n",
        "    print(f\"[Data]: Reading data...\\n\")\n",
        "\n",
        "    # Importing the raw dataset\n",
        "    dataframe = dataframe[[source_text, target_text]]\n",
        "\n",
        "    # Creation of Dataset and Dataloader\n",
        "    # Defining the train size. So 80% of the data will be used for training and the rest for validation.\n",
        "    train_size = 0.8\n",
        "    train_dataset = dataframe.sample(frac=train_size, random_state=model_params[\"SEED\"])\n",
        "    val_dataset = dataframe.drop(train_dataset.index).reset_index(drop=True)\n",
        "    train_dataset = train_dataset.reset_index(drop=True)\n",
        "\n",
        "    print(f\"FULL Dataset: {dataframe.shape}\")\n",
        "    print(f\"TRAIN Dataset: {train_dataset.shape}\")\n",
        "    print(f\"TEST Dataset: {val_dataset.shape}\\n\")\n",
        "\n",
        "    # Creating the Training and Validation dataset for further creation of Dataloader\n",
        "    training_set = Chatbot(\n",
        "        train_dataset,\n",
        "        tokenizer,\n",
        "        model_params[\"MAX_SOURCE_TEXT_LENGTH\"],\n",
        "        model_params[\"MAX_TARGET_TEXT_LENGTH\"],\n",
        "        source_text,\n",
        "        target_text,\n",
        "    )\n",
        "    val_set = Chatbot(\n",
        "        val_dataset,\n",
        "        tokenizer,\n",
        "        model_params[\"MAX_SOURCE_TEXT_LENGTH\"],\n",
        "        model_params[\"MAX_TARGET_TEXT_LENGTH\"],\n",
        "        source_text,\n",
        "        target_text,\n",
        "    )\n",
        "\n",
        "    # Defining the parameters for creation of dataloaders\n",
        "    train_params = {\n",
        "        \"batch_size\": model_params[\"TRAIN_BATCH_SIZE\"],\n",
        "        \"shuffle\": True,\n",
        "        \"num_workers\": 0,\n",
        "    }\n",
        "\n",
        "    val_params = {\n",
        "        \"batch_size\": model_params[\"VALID_BATCH_SIZE\"],\n",
        "        \"shuffle\": False,\n",
        "        \"num_workers\": 0,\n",
        "    }\n",
        "\n",
        "    # Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\n",
        "    training_loader = DataLoader(training_set, **train_params)\n",
        "    val_loader = DataLoader(val_set, **val_params)\n",
        "\n",
        "    # Defining the optimizer that will be used to tune the weights of the network in the training session.\n",
        "    optimizer = torch.optim.Adam(\n",
        "        params=model.parameters(), lr=model_params[\"LEARNING_RATE\"]\n",
        "    )\n",
        "\n",
        "    # Training loop\n",
        "    print(f\"[Initiating Fine Tuning]...\\n\")\n",
        "\n",
        "    for epoch in range(model_params[\"TRAIN_EPOCHS\"]):\n",
        "        train(epoch, tokenizer, model, device, training_loader, optimizer)\n",
        "\n",
        "    # Saving the model after training\n",
        "    path = os.path.join(output_dir, \"model_files\")\n",
        "    model.save_pretrained(path)\n",
        "    tokenizer.save_pretrained(path)\n",
        "\n",
        "    # evaluating test dataset\n",
        "    print(f\"[Initiating Validation]...\\n\")\n",
        "    for epoch in range(model_params[\"VAL_EPOCHS\"]):\n",
        "        predictions, actuals = validate(epoch, tokenizer, model, device, val_loader)\n",
        "        final_df = pd.DataFrame({\"Generated Text\": predictions, \"Actual Text\": actuals})\n",
        "        final_df.to_csv(os.path.join(output_dir, \"predictions.csv\"))\n",
        "\n",
        "    print(f\"[Validation Completed.]\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "9i49Ggjz_nxV"
      },
      "outputs": [],
      "source": [
        "model_params = {\n",
        "    \"MODEL\": \"google/mt5-small\",  # model_type: t5-base/t5-large\n",
        "    \"TRAIN_BATCH_SIZE\": 4,  # training batch size\n",
        "    \"VALID_BATCH_SIZE\": 4,  # validation batch size\n",
        "    \"TRAIN_EPOCHS\": 10,  # number of training epochs\n",
        "    \"VAL_EPOCHS\": 1,  # number of validation epochs\n",
        "    \"LEARNING_RATE\": 1e-4,  # learning rate\n",
        "    \"MAX_SOURCE_TEXT_LENGTH\": 512,  # max length of source text\n",
        "    \"MAX_TARGET_TEXT_LENGTH\": 256,  # max length of target text\n",
        "    \"SEED\": 2022,  # set seed for reproducibility\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "S8PQTwEVAyIy"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('Script_for_all.csv')\n",
        "df.columns = ['Question','Answer']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uVkLSIJi_4ae",
        "outputId": "83b6cfb7-4773-46ac-a2d1-157ce9dc3afb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Model]: Loading google/mt5-small...\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You are using a model of type mt5 to instantiate a model of type t5. This is not supported for all configurations of models and can yield errors.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Data]: Reading data...\n",
            "\n",
            "FULL Dataset: (10356, 2)\n",
            "TRAIN Dataset: (8285, 2)\n",
            "TEST Dataset: (2071, 2)\n",
            "\n",
            "[Initiating Fine Tuning]...\n",
            "\n",
            "idx: 0 epoch: 0 loss: tensor(16.9405, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 10 epoch: 0 loss: tensor(15.1923, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 20 epoch: 0 loss: tensor(10.1822, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 30 epoch: 0 loss: tensor(10.7462, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 40 epoch: 0 loss: tensor(9.1641, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 50 epoch: 0 loss: tensor(9.5966, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 60 epoch: 0 loss: tensor(9.0957, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 70 epoch: 0 loss: tensor(9.0463, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 80 epoch: 0 loss: tensor(8.4125, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 90 epoch: 0 loss: tensor(8.0348, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 100 epoch: 0 loss: tensor(8.9803, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 110 epoch: 0 loss: tensor(8.6096, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 120 epoch: 0 loss: tensor(7.7103, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 130 epoch: 0 loss: tensor(8.0129, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 140 epoch: 0 loss: tensor(7.3152, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 150 epoch: 0 loss: tensor(8.1111, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 160 epoch: 0 loss: tensor(7.8431, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 170 epoch: 0 loss: tensor(8.0282, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 180 epoch: 0 loss: tensor(7.5243, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 190 epoch: 0 loss: tensor(7.4361, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 200 epoch: 0 loss: tensor(7.6572, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 210 epoch: 0 loss: tensor(8.3904, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 220 epoch: 0 loss: tensor(7.5824, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 230 epoch: 0 loss: tensor(6.4325, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 240 epoch: 0 loss: tensor(7.6334, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 250 epoch: 0 loss: tensor(7.6527, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 260 epoch: 0 loss: tensor(6.8756, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 270 epoch: 0 loss: tensor(6.2899, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 280 epoch: 0 loss: tensor(6.8271, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 290 epoch: 0 loss: tensor(5.9347, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 300 epoch: 0 loss: tensor(7.0842, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 310 epoch: 0 loss: tensor(6.3651, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 320 epoch: 0 loss: tensor(6.7571, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 330 epoch: 0 loss: tensor(6.6709, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 340 epoch: 0 loss: tensor(7.1566, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 350 epoch: 0 loss: tensor(5.8532, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 360 epoch: 0 loss: tensor(5.9466, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 370 epoch: 0 loss: tensor(6.5844, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 380 epoch: 0 loss: tensor(6.0279, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 390 epoch: 0 loss: tensor(7.5450, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 400 epoch: 0 loss: tensor(6.0961, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 410 epoch: 0 loss: tensor(6.0352, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 420 epoch: 0 loss: tensor(6.1130, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 430 epoch: 0 loss: tensor(5.8127, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 440 epoch: 0 loss: tensor(6.5266, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 450 epoch: 0 loss: tensor(6.2656, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 460 epoch: 0 loss: tensor(6.2908, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 470 epoch: 0 loss: tensor(6.0355, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 480 epoch: 0 loss: tensor(6.2526, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 490 epoch: 0 loss: tensor(6.1491, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 500 epoch: 0 loss: tensor(6.3500, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 510 epoch: 0 loss: tensor(6.4547, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 520 epoch: 0 loss: tensor(6.3099, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 530 epoch: 0 loss: tensor(5.9704, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 540 epoch: 0 loss: tensor(6.7218, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 550 epoch: 0 loss: tensor(7.1384, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 560 epoch: 0 loss: tensor(5.7342, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 570 epoch: 0 loss: tensor(5.7755, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 580 epoch: 0 loss: tensor(6.0646, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 590 epoch: 0 loss: tensor(5.9600, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 600 epoch: 0 loss: tensor(5.9766, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 610 epoch: 0 loss: tensor(5.4984, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 620 epoch: 0 loss: tensor(6.5096, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 630 epoch: 0 loss: tensor(5.9956, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 640 epoch: 0 loss: tensor(5.5226, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 650 epoch: 0 loss: tensor(5.3277, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 660 epoch: 0 loss: tensor(4.9648, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 670 epoch: 0 loss: tensor(6.3888, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 680 epoch: 0 loss: tensor(6.1195, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 690 epoch: 0 loss: tensor(5.0455, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 700 epoch: 0 loss: tensor(5.6162, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 710 epoch: 0 loss: tensor(5.6241, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 720 epoch: 0 loss: tensor(4.6823, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 730 epoch: 0 loss: tensor(5.7976, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 740 epoch: 0 loss: tensor(6.5854, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 750 epoch: 0 loss: tensor(5.8691, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 760 epoch: 0 loss: tensor(6.2833, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 770 epoch: 0 loss: tensor(5.2286, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 780 epoch: 0 loss: tensor(5.9985, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 790 epoch: 0 loss: tensor(5.3321, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 800 epoch: 0 loss: tensor(5.8856, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 810 epoch: 0 loss: tensor(4.9729, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 820 epoch: 0 loss: tensor(5.0550, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 830 epoch: 0 loss: tensor(5.5007, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 840 epoch: 0 loss: tensor(5.5746, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 850 epoch: 0 loss: tensor(5.3999, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 860 epoch: 0 loss: tensor(6.0010, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 870 epoch: 0 loss: tensor(5.5331, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 880 epoch: 0 loss: tensor(5.4349, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 890 epoch: 0 loss: tensor(5.3112, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 900 epoch: 0 loss: tensor(5.1996, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 910 epoch: 0 loss: tensor(5.7792, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 920 epoch: 0 loss: tensor(5.0282, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 930 epoch: 0 loss: tensor(5.3206, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 940 epoch: 0 loss: tensor(5.2334, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 950 epoch: 0 loss: tensor(5.9658, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 960 epoch: 0 loss: tensor(5.3168, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 970 epoch: 0 loss: tensor(5.3537, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 980 epoch: 0 loss: tensor(6.1358, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 990 epoch: 0 loss: tensor(5.5294, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1000 epoch: 0 loss: tensor(6.2863, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1010 epoch: 0 loss: tensor(5.0606, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1020 epoch: 0 loss: tensor(5.4569, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1030 epoch: 0 loss: tensor(5.2648, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1040 epoch: 0 loss: tensor(4.9484, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1050 epoch: 0 loss: tensor(5.4434, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1060 epoch: 0 loss: tensor(5.6606, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1070 epoch: 0 loss: tensor(5.6300, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1080 epoch: 0 loss: tensor(5.6125, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1090 epoch: 0 loss: tensor(5.5683, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1100 epoch: 0 loss: tensor(5.1127, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1110 epoch: 0 loss: tensor(5.5760, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1120 epoch: 0 loss: tensor(5.5153, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1130 epoch: 0 loss: tensor(5.2983, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1140 epoch: 0 loss: tensor(5.2401, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1150 epoch: 0 loss: tensor(5.5902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1160 epoch: 0 loss: tensor(5.4392, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1170 epoch: 0 loss: tensor(5.3389, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1180 epoch: 0 loss: tensor(5.8364, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1190 epoch: 0 loss: tensor(5.0568, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1200 epoch: 0 loss: tensor(5.1770, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1210 epoch: 0 loss: tensor(5.0267, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1220 epoch: 0 loss: tensor(5.7901, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1230 epoch: 0 loss: tensor(5.3405, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1240 epoch: 0 loss: tensor(5.0161, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1250 epoch: 0 loss: tensor(5.6113, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1260 epoch: 0 loss: tensor(5.2444, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1270 epoch: 0 loss: tensor(5.3317, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1280 epoch: 0 loss: tensor(5.1306, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1290 epoch: 0 loss: tensor(5.4637, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1300 epoch: 0 loss: tensor(4.9751, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1310 epoch: 0 loss: tensor(5.6824, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1320 epoch: 0 loss: tensor(4.9751, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1330 epoch: 0 loss: tensor(4.5648, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1340 epoch: 0 loss: tensor(5.7342, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1350 epoch: 0 loss: tensor(5.5416, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1360 epoch: 0 loss: tensor(5.9880, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1370 epoch: 0 loss: tensor(5.2587, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1380 epoch: 0 loss: tensor(4.7274, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1390 epoch: 0 loss: tensor(5.1919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1400 epoch: 0 loss: tensor(5.1547, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1410 epoch: 0 loss: tensor(5.4800, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1420 epoch: 0 loss: tensor(5.5721, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1430 epoch: 0 loss: tensor(4.5241, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1440 epoch: 0 loss: tensor(5.2987, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1450 epoch: 0 loss: tensor(5.6289, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1460 epoch: 0 loss: tensor(5.9897, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1470 epoch: 0 loss: tensor(5.4146, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1480 epoch: 0 loss: tensor(5.3626, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1490 epoch: 0 loss: tensor(5.1264, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1500 epoch: 0 loss: tensor(5.1457, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1510 epoch: 0 loss: tensor(5.3924, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1520 epoch: 0 loss: tensor(5.4065, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1530 epoch: 0 loss: tensor(5.4062, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1540 epoch: 0 loss: tensor(4.9008, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1550 epoch: 0 loss: tensor(5.2416, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1560 epoch: 0 loss: tensor(5.7217, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1570 epoch: 0 loss: tensor(5.0096, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1580 epoch: 0 loss: tensor(4.9292, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1590 epoch: 0 loss: tensor(5.3220, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1600 epoch: 0 loss: tensor(5.0375, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1610 epoch: 0 loss: tensor(4.5917, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1620 epoch: 0 loss: tensor(4.8144, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1630 epoch: 0 loss: tensor(5.2782, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1640 epoch: 0 loss: tensor(5.8420, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1650 epoch: 0 loss: tensor(5.2082, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1660 epoch: 0 loss: tensor(5.6938, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1670 epoch: 0 loss: tensor(4.8189, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1680 epoch: 0 loss: tensor(5.4128, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1690 epoch: 0 loss: tensor(4.9304, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1700 epoch: 0 loss: tensor(5.1733, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1710 epoch: 0 loss: tensor(5.2247, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1720 epoch: 0 loss: tensor(4.9918, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1730 epoch: 0 loss: tensor(5.2061, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1740 epoch: 0 loss: tensor(5.2111, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1750 epoch: 0 loss: tensor(4.6973, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1760 epoch: 0 loss: tensor(5.5271, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1770 epoch: 0 loss: tensor(4.9441, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1780 epoch: 0 loss: tensor(4.9062, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1790 epoch: 0 loss: tensor(4.8208, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1800 epoch: 0 loss: tensor(5.3836, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1810 epoch: 0 loss: tensor(5.5771, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1820 epoch: 0 loss: tensor(5.3388, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1830 epoch: 0 loss: tensor(4.7460, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1840 epoch: 0 loss: tensor(5.3755, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1850 epoch: 0 loss: tensor(4.9788, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1860 epoch: 0 loss: tensor(4.5697, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1870 epoch: 0 loss: tensor(4.8089, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1880 epoch: 0 loss: tensor(4.9238, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1890 epoch: 0 loss: tensor(4.1651, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1900 epoch: 0 loss: tensor(4.7688, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1910 epoch: 0 loss: tensor(5.0431, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1920 epoch: 0 loss: tensor(5.6700, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1930 epoch: 0 loss: tensor(4.7164, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1940 epoch: 0 loss: tensor(4.4155, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1950 epoch: 0 loss: tensor(4.5506, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1960 epoch: 0 loss: tensor(4.6360, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1970 epoch: 0 loss: tensor(4.9134, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1980 epoch: 0 loss: tensor(5.0701, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1990 epoch: 0 loss: tensor(4.1996, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 2000 epoch: 0 loss: tensor(5.2287, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 2010 epoch: 0 loss: tensor(5.2284, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 2020 epoch: 0 loss: tensor(5.1190, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 2030 epoch: 0 loss: tensor(5.0450, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 2040 epoch: 0 loss: tensor(4.8344, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 2050 epoch: 0 loss: tensor(4.9777, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 2060 epoch: 0 loss: tensor(5.0329, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 2070 epoch: 0 loss: tensor(5.1598, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 0 epoch: 1 loss: tensor(4.8453, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 10 epoch: 1 loss: tensor(4.6962, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 20 epoch: 1 loss: tensor(5.1121, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 30 epoch: 1 loss: tensor(5.1195, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 40 epoch: 1 loss: tensor(5.0014, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 50 epoch: 1 loss: tensor(4.6162, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 60 epoch: 1 loss: tensor(4.7396, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 70 epoch: 1 loss: tensor(5.3972, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 80 epoch: 1 loss: tensor(4.4137, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 90 epoch: 1 loss: tensor(5.4657, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 100 epoch: 1 loss: tensor(5.3853, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 110 epoch: 1 loss: tensor(5.3871, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 120 epoch: 1 loss: tensor(4.9805, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 130 epoch: 1 loss: tensor(5.2414, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 140 epoch: 1 loss: tensor(4.8523, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 150 epoch: 1 loss: tensor(4.9307, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 160 epoch: 1 loss: tensor(4.7212, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 170 epoch: 1 loss: tensor(4.7976, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 180 epoch: 1 loss: tensor(5.2668, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 190 epoch: 1 loss: tensor(4.8451, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 200 epoch: 1 loss: tensor(4.8879, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 210 epoch: 1 loss: tensor(5.0390, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 220 epoch: 1 loss: tensor(4.8177, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 230 epoch: 1 loss: tensor(5.1523, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 240 epoch: 1 loss: tensor(4.2947, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 250 epoch: 1 loss: tensor(5.0309, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 260 epoch: 1 loss: tensor(4.9797, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 270 epoch: 1 loss: tensor(4.4549, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 280 epoch: 1 loss: tensor(5.6525, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 290 epoch: 1 loss: tensor(4.3445, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 300 epoch: 1 loss: tensor(5.3514, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 310 epoch: 1 loss: tensor(4.5968, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 320 epoch: 1 loss: tensor(4.3286, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 330 epoch: 1 loss: tensor(5.0494, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 340 epoch: 1 loss: tensor(4.8913, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 350 epoch: 1 loss: tensor(4.7709, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 360 epoch: 1 loss: tensor(5.1636, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 370 epoch: 1 loss: tensor(5.1948, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 380 epoch: 1 loss: tensor(4.4989, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 390 epoch: 1 loss: tensor(4.7939, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 400 epoch: 1 loss: tensor(4.7506, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 410 epoch: 1 loss: tensor(5.1856, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 420 epoch: 1 loss: tensor(4.9611, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 430 epoch: 1 loss: tensor(4.3300, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 440 epoch: 1 loss: tensor(5.2141, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 450 epoch: 1 loss: tensor(5.4662, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 460 epoch: 1 loss: tensor(4.5333, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 470 epoch: 1 loss: tensor(5.7353, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 480 epoch: 1 loss: tensor(5.0989, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 490 epoch: 1 loss: tensor(4.7987, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 500 epoch: 1 loss: tensor(5.3894, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 510 epoch: 1 loss: tensor(4.5543, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 520 epoch: 1 loss: tensor(4.3984, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 530 epoch: 1 loss: tensor(4.7926, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 540 epoch: 1 loss: tensor(4.9043, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 550 epoch: 1 loss: tensor(5.3968, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 560 epoch: 1 loss: tensor(4.6498, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 570 epoch: 1 loss: tensor(5.5163, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 580 epoch: 1 loss: tensor(4.9083, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 590 epoch: 1 loss: tensor(4.5480, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 600 epoch: 1 loss: tensor(4.9308, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 610 epoch: 1 loss: tensor(4.3511, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 620 epoch: 1 loss: tensor(4.9783, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 630 epoch: 1 loss: tensor(4.7287, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 640 epoch: 1 loss: tensor(4.8787, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 650 epoch: 1 loss: tensor(3.9901, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 660 epoch: 1 loss: tensor(4.8492, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 670 epoch: 1 loss: tensor(4.6104, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 680 epoch: 1 loss: tensor(4.8028, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 690 epoch: 1 loss: tensor(5.2845, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 700 epoch: 1 loss: tensor(4.9670, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 710 epoch: 1 loss: tensor(4.2441, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 720 epoch: 1 loss: tensor(4.3918, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 730 epoch: 1 loss: tensor(4.8364, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 740 epoch: 1 loss: tensor(4.9695, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 750 epoch: 1 loss: tensor(4.2812, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 760 epoch: 1 loss: tensor(4.8376, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 770 epoch: 1 loss: tensor(4.5671, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 780 epoch: 1 loss: tensor(4.5086, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 790 epoch: 1 loss: tensor(4.6693, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 800 epoch: 1 loss: tensor(4.8611, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 810 epoch: 1 loss: tensor(5.2808, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 820 epoch: 1 loss: tensor(4.3521, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 830 epoch: 1 loss: tensor(4.6544, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 840 epoch: 1 loss: tensor(4.6408, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 850 epoch: 1 loss: tensor(4.1134, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 860 epoch: 1 loss: tensor(4.6362, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 870 epoch: 1 loss: tensor(4.9017, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 880 epoch: 1 loss: tensor(4.6993, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 890 epoch: 1 loss: tensor(4.6102, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 900 epoch: 1 loss: tensor(5.1457, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 910 epoch: 1 loss: tensor(4.5666, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 920 epoch: 1 loss: tensor(4.4639, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 930 epoch: 1 loss: tensor(5.2792, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 940 epoch: 1 loss: tensor(4.7687, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 950 epoch: 1 loss: tensor(5.1695, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 960 epoch: 1 loss: tensor(3.9374, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 970 epoch: 1 loss: tensor(4.3081, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 980 epoch: 1 loss: tensor(4.4038, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 990 epoch: 1 loss: tensor(4.5214, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1000 epoch: 1 loss: tensor(5.2956, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1010 epoch: 1 loss: tensor(4.4614, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1020 epoch: 1 loss: tensor(4.7530, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1030 epoch: 1 loss: tensor(4.8231, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1040 epoch: 1 loss: tensor(5.2829, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1050 epoch: 1 loss: tensor(4.4009, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1060 epoch: 1 loss: tensor(4.6180, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1070 epoch: 1 loss: tensor(5.0349, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1080 epoch: 1 loss: tensor(4.5653, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1090 epoch: 1 loss: tensor(4.7909, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1100 epoch: 1 loss: tensor(5.4236, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1110 epoch: 1 loss: tensor(4.8278, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1120 epoch: 1 loss: tensor(4.3121, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1130 epoch: 1 loss: tensor(4.2715, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1140 epoch: 1 loss: tensor(4.1621, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1150 epoch: 1 loss: tensor(4.5467, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1160 epoch: 1 loss: tensor(4.9675, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1170 epoch: 1 loss: tensor(4.8536, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1180 epoch: 1 loss: tensor(4.1444, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1190 epoch: 1 loss: tensor(5.3189, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1200 epoch: 1 loss: tensor(5.2043, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1210 epoch: 1 loss: tensor(4.6095, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1220 epoch: 1 loss: tensor(4.2454, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1230 epoch: 1 loss: tensor(4.4520, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1240 epoch: 1 loss: tensor(4.1023, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1250 epoch: 1 loss: tensor(4.6212, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1260 epoch: 1 loss: tensor(4.4337, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1270 epoch: 1 loss: tensor(3.8815, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1280 epoch: 1 loss: tensor(4.6277, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1290 epoch: 1 loss: tensor(4.6175, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1300 epoch: 1 loss: tensor(4.2716, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1310 epoch: 1 loss: tensor(4.4781, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1320 epoch: 1 loss: tensor(5.0981, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1330 epoch: 1 loss: tensor(4.8302, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1340 epoch: 1 loss: tensor(5.0465, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1350 epoch: 1 loss: tensor(4.6888, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1360 epoch: 1 loss: tensor(4.7917, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1370 epoch: 1 loss: tensor(4.8348, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1380 epoch: 1 loss: tensor(4.8901, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1390 epoch: 1 loss: tensor(4.4494, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1400 epoch: 1 loss: tensor(3.7608, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1410 epoch: 1 loss: tensor(3.4716, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1420 epoch: 1 loss: tensor(4.9895, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1430 epoch: 1 loss: tensor(5.3502, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1440 epoch: 1 loss: tensor(4.4262, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1450 epoch: 1 loss: tensor(4.0701, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1460 epoch: 1 loss: tensor(5.1108, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1470 epoch: 1 loss: tensor(4.4532, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1480 epoch: 1 loss: tensor(4.0912, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1490 epoch: 1 loss: tensor(4.4115, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1500 epoch: 1 loss: tensor(5.3856, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1510 epoch: 1 loss: tensor(4.4502, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1520 epoch: 1 loss: tensor(5.4248, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1530 epoch: 1 loss: tensor(4.7475, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1540 epoch: 1 loss: tensor(4.7324, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1550 epoch: 1 loss: tensor(5.2127, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1560 epoch: 1 loss: tensor(4.4607, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1570 epoch: 1 loss: tensor(3.9922, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1580 epoch: 1 loss: tensor(4.3414, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1590 epoch: 1 loss: tensor(5.0017, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1600 epoch: 1 loss: tensor(4.5310, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1610 epoch: 1 loss: tensor(3.3120, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1620 epoch: 1 loss: tensor(4.7604, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1630 epoch: 1 loss: tensor(4.7295, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1640 epoch: 1 loss: tensor(4.8201, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1650 epoch: 1 loss: tensor(4.2952, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1660 epoch: 1 loss: tensor(4.2820, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1670 epoch: 1 loss: tensor(4.0657, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1680 epoch: 1 loss: tensor(4.6708, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1690 epoch: 1 loss: tensor(4.8199, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1700 epoch: 1 loss: tensor(3.3551, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1710 epoch: 1 loss: tensor(4.8076, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1720 epoch: 1 loss: tensor(4.4915, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1730 epoch: 1 loss: tensor(4.9054, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1740 epoch: 1 loss: tensor(4.6855, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1750 epoch: 1 loss: tensor(4.1147, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1760 epoch: 1 loss: tensor(4.6960, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1770 epoch: 1 loss: tensor(4.0531, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1780 epoch: 1 loss: tensor(4.5486, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1790 epoch: 1 loss: tensor(4.6519, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1800 epoch: 1 loss: tensor(4.1484, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1810 epoch: 1 loss: tensor(4.3076, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1820 epoch: 1 loss: tensor(4.1856, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1830 epoch: 1 loss: tensor(5.3180, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1840 epoch: 1 loss: tensor(5.0103, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1850 epoch: 1 loss: tensor(4.8227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1860 epoch: 1 loss: tensor(4.2177, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1870 epoch: 1 loss: tensor(4.4877, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1880 epoch: 1 loss: tensor(3.5224, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1890 epoch: 1 loss: tensor(5.1087, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1900 epoch: 1 loss: tensor(4.5689, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1910 epoch: 1 loss: tensor(4.8420, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1920 epoch: 1 loss: tensor(4.4543, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1930 epoch: 1 loss: tensor(4.2970, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1940 epoch: 1 loss: tensor(3.7588, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1950 epoch: 1 loss: tensor(4.4560, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1960 epoch: 1 loss: tensor(4.2880, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1970 epoch: 1 loss: tensor(3.9270, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1980 epoch: 1 loss: tensor(4.4973, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 1990 epoch: 1 loss: tensor(4.3954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 2000 epoch: 1 loss: tensor(4.8138, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 2010 epoch: 1 loss: tensor(4.7597, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 2020 epoch: 1 loss: tensor(4.6022, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 2030 epoch: 1 loss: tensor(4.4861, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 2040 epoch: 1 loss: tensor(3.9180, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 2050 epoch: 1 loss: tensor(4.8535, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 2060 epoch: 1 loss: tensor(5.1462, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 2070 epoch: 1 loss: tensor(5.2202, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 0 epoch: 2 loss: tensor(4.0552, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 10 epoch: 2 loss: tensor(4.9875, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 20 epoch: 2 loss: tensor(4.1790, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 30 epoch: 2 loss: tensor(4.5029, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 40 epoch: 2 loss: tensor(4.9553, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 50 epoch: 2 loss: tensor(4.5571, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 60 epoch: 2 loss: tensor(4.3774, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 70 epoch: 2 loss: tensor(3.9881, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 80 epoch: 2 loss: tensor(4.2458, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 90 epoch: 2 loss: tensor(4.5395, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 100 epoch: 2 loss: tensor(4.5134, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 110 epoch: 2 loss: tensor(4.9794, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 120 epoch: 2 loss: tensor(4.4734, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 130 epoch: 2 loss: tensor(4.4418, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 140 epoch: 2 loss: tensor(4.8850, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 150 epoch: 2 loss: tensor(3.8339, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 160 epoch: 2 loss: tensor(5.2467, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 170 epoch: 2 loss: tensor(4.6545, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 180 epoch: 2 loss: tensor(4.4574, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 190 epoch: 2 loss: tensor(5.0341, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 200 epoch: 2 loss: tensor(4.8948, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 210 epoch: 2 loss: tensor(5.0064, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 220 epoch: 2 loss: tensor(4.8050, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 230 epoch: 2 loss: tensor(4.7766, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "idx: 240 epoch: 2 loss: tensor(5.0022, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-aa791ad80237>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtarget_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Answer\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mmodel_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0moutput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"outputs\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m )\n",
            "\u001b[0;32m<ipython-input-5-c85e3b0438bf>\u001b[0m in \u001b[0;36mT5Trainer\u001b[0;34m(dataframe, source_text, target_text, model_params, output_dir)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"TRAIN_EPOCHS\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;31m# Saving the model after training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-09a140193c57>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, tokenizer, model, device, loader, optimizer)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "df[\"Question\"] = \"Question: \" + df[\"Question\"]\n",
        "\n",
        "T5Trainer(\n",
        "    dataframe=df,\n",
        "    source_text=\"Question\",\n",
        "    target_text=\"Answer\",\n",
        "    model_params=model_params,\n",
        "    output_dir=\"outputs\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mw1AkVj8O8aH"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "T5.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
